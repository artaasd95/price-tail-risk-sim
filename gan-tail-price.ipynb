{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import numpy as np\n",
    "\n",
    "from models.gan_models import *\n",
    "import yfinance as yf\n",
    "import MetaTrader5 as mt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/artaasd95/tail-price/eb48bfc14f224b4b934b3473e5a7fed5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "  api_key=\"vpNJF6XOWcHS6HqH9ZFEjwRcD\",\n",
    "  project_name=\"tail-price\",\n",
    "  workspace=\"artaasd95\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading current tf data\n"
     ]
    }
   ],
   "source": [
    "mt5.initialize()\n",
    "\n",
    "print('loading current tf data')\n",
    "utc_from = datetime(2021, 1, 1, tzinfo=pytz.timezone(\"Asia/Nicosia\"))\n",
    "utc_to = datetime.now(pytz.timezone(\"Asia/Nicosia\"))\n",
    "\n",
    "data = mt5.copy_rates_range('XAUUSD', mt5.TIMEFRAME_H4, utc_from, utc_to)\n",
    "data = pd.DataFrame(data)\n",
    "time_data = data.time\n",
    "data.drop(columns=['tick_volume', 'spread', 'real_volume'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('xau_2021_H4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('xau_2021_H4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1609718400</td>\n",
       "      <td>1904.48</td>\n",
       "      <td>1944.33</td>\n",
       "      <td>1900.62</td>\n",
       "      <td>1943.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1609804800</td>\n",
       "      <td>1941.22</td>\n",
       "      <td>1952.90</td>\n",
       "      <td>1934.13</td>\n",
       "      <td>1949.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1609891200</td>\n",
       "      <td>1948.82</td>\n",
       "      <td>1959.30</td>\n",
       "      <td>1900.65</td>\n",
       "      <td>1918.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1609977600</td>\n",
       "      <td>1917.82</td>\n",
       "      <td>1927.54</td>\n",
       "      <td>1906.69</td>\n",
       "      <td>1913.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1610064000</td>\n",
       "      <td>1913.52</td>\n",
       "      <td>1917.38</td>\n",
       "      <td>1828.05</td>\n",
       "      <td>1848.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>932</td>\n",
       "      <td>1723420800</td>\n",
       "      <td>2428.05</td>\n",
       "      <td>2472.17</td>\n",
       "      <td>2423.83</td>\n",
       "      <td>2472.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>933</td>\n",
       "      <td>1723507200</td>\n",
       "      <td>2470.68</td>\n",
       "      <td>2476.97</td>\n",
       "      <td>2458.46</td>\n",
       "      <td>2465.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>934</td>\n",
       "      <td>1723593600</td>\n",
       "      <td>2466.51</td>\n",
       "      <td>2479.94</td>\n",
       "      <td>2438.05</td>\n",
       "      <td>2447.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>935</td>\n",
       "      <td>1723680000</td>\n",
       "      <td>2446.85</td>\n",
       "      <td>2470.08</td>\n",
       "      <td>2432.15</td>\n",
       "      <td>2456.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>936</td>\n",
       "      <td>1723766400</td>\n",
       "      <td>2454.92</td>\n",
       "      <td>2509.68</td>\n",
       "      <td>2450.72</td>\n",
       "      <td>2506.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        time     open     high      low    close\n",
       "0             0  1609718400  1904.48  1944.33  1900.62  1943.10\n",
       "1             1  1609804800  1941.22  1952.90  1934.13  1949.87\n",
       "2             2  1609891200  1948.82  1959.30  1900.65  1918.62\n",
       "3             3  1609977600  1917.82  1927.54  1906.69  1913.35\n",
       "4             4  1610064000  1913.52  1917.38  1828.05  1848.58\n",
       "..          ...         ...      ...      ...      ...      ...\n",
       "932         932  1723420800  2428.05  2472.17  2423.83  2472.06\n",
       "933         933  1723507200  2470.68  2476.97  2458.46  2465.41\n",
       "934         934  1723593600  2466.51  2479.94  2438.05  2447.66\n",
       "935         935  1723680000  2446.85  2470.08  2432.15  2456.19\n",
       "936         936  1723766400  2454.92  2509.68  2450.72  2506.82\n",
       "\n",
       "[937 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 1024\n",
    "seq_length = 50\n",
    "num_layers = 10\n",
    "batch_size = 32\n",
    "num_epochs = 150\n",
    "learning_rate = 0.3\n",
    "\n",
    "# Initialize models\n",
    "main_gen = MainGenerator(input_size, hidden_size, num_layers, 0.5, 1)\n",
    "noise_gen = NoiseGenerator(input_size, 1024, 1, 0.3)\n",
    "discriminator = Discriminator(input_size, hidden_size, 15, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (lstm): LSTM(1, 1024, num_layers=15, batch_first=True, dropout=0.3)\n",
       "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_gen.to(device)\n",
    "noise_gen.to(device)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.AdamW(list(main_gen.parameters()) + list(noise_gen.parameters()), lr=learning_rate)\n",
    "optimizer_D = torch.optim.AdamW(discriminator.parameters(), lr=learning_rate)\n",
    "optimizer_noise = torch.optim.AdamW(noise_gen.parameters(), lr=learning_rate)\n",
    "\n",
    "optim_g_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer_G, 0.1)\n",
    "optim_d_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer_D, 0.1)\n",
    "optim_noise_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer_noise, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cauchy_dist = torch.distributions.cauchy.Cauchy(loc=0, scale=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = F.kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:int(len(data)*0.7)]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = data[int(len(data)*0.7):]\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]c:\\Users\\arta\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\functional.py:2976: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/150 [01:46<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150]  Discriminator Loss: -1847683840.0000  Generator Loss: -5249550.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m optim_noise_sched\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  Discriminator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  Generator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m experiment\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Generator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39maverage(g_loss_list), step\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[0;32m     52\u001b[0m experiment\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoise Generator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39maverage(noise_loss_list), step\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[0;32m     53\u001b[0m experiment\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiscriminator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39maverage(d_loss_list), step\u001b[38;5;241m=\u001b[39mepoch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    batch_start = 0\n",
    "    total_loss_list = []\n",
    "    g_loss_list = []\n",
    "    noise_loss_list = []\n",
    "    d_loss_list = []\n",
    "    for idx in range(int(len(train_data)/batch_size)-1):\n",
    "        price = torch.tensor(train_data[batch_start:batch_start+batch_size].close.values, dtype=torch.float).reshape(batch_size,1)\n",
    "        price = price.to(device)\n",
    "        batch_start = batch_start + batch_size\n",
    "        # Generate random noise inputs\n",
    "        z1 = cauchy_dist.sample([batch_size, 1])\n",
    "        z1 = z1.to(device)\n",
    "        #z2 = cauchy_dist.sample([1])\n",
    "        \n",
    "        # Generate fake data\n",
    "        fake_main = main_gen(price)\n",
    "        fake_noise = noise_gen(z1)\n",
    "        fake_data = fake_main + fake_noise\n",
    "        \n",
    "        # Train discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss = adversarial_loss(discriminator(fake_data.detach()), price, reduction='batchmean')\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train generators\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_main = main_gen(price)\n",
    "        fake_noise = noise_gen(z1)\n",
    "        fake_data = fake_main + fake_noise\n",
    "        d_loss = adversarial_loss(F.log_softmax(discriminator(fake_data)), F.softmax(price), reduction='batchmean')\n",
    "        g_loss = adversarial_loss(F.log_softmax(fake_data), F.softmax(price), reduction='batchmean')\n",
    "        noise_loss = adversarial_loss(F.log_softmax(fake_noise), F.softmax(z1), reduction='batchmean')\n",
    "        total_loss = 0.5 * ((0.8 * g_loss + 0.2 * noise_loss) + d_loss)\n",
    "        total_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_noise.step()\n",
    "\n",
    "        total_loss_list.append(total_loss.item())\n",
    "        g_loss_list.append(g_loss.item())\n",
    "        noise_loss_list.append(noise_loss.item())\n",
    "        d_loss_list.append(d_loss.item())\n",
    "\n",
    "    optim_g_sched.step()\n",
    "    optim_d_sched.step()\n",
    "    optim_noise_sched.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]  Discriminator Loss: {d_loss.item():.4f}  Generator Loss: {g_loss.item():.4f}\")\n",
    "    experiment.log_metric('Main Generator\\Train', np.average(g_loss_list), step=epoch)\n",
    "    experiment.log_metric('Noise Generator\\Train', np.average(noise_loss_list), step=epoch)\n",
    "    experiment.log_metric('Discriminator\\Train', np.average(d_loss_list), step=epoch)\n",
    "    experiment.log_metric('Total\\Train', np.average(total_loss_list), step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "batch_start = 0\n",
    "for idx in range(int(len(test_data)/batch_size)-1):\n",
    "    price = torch.tensor(test_data[batch_start:batch_start+batch_size].close, dtype=torch.float).reshape(batch_size,1)\n",
    "    price = price.to(device)\n",
    "    batch_start = batch_start + batch_size\n",
    "    # Generate random noise inputs\n",
    "    z1 = cauchy_dist.sample([batch_size, 1])\n",
    "    z1 = z1.to(device)\n",
    "    #z2 = cauchy_dist.sample([1])\n",
    "    main_gen.eval()\n",
    "    noise_gen.eval()\n",
    "    discriminator.eval()\n",
    "    # Generate fake data\n",
    "    fake_main = main_gen(price)\n",
    "    fake_noise = noise_gen(z1)\n",
    "    fake_data = fake_main + fake_noise\n",
    "\n",
    "\n",
    "    d_loss = adversarial_loss(F.log_softmax(discriminator(fake_data)), F.softmax(price), reduction='batchmean')\n",
    "    g_loss = adversarial_loss(F.log_softmax(fake_data), F.softmax(price), reduction='batchmean')\n",
    "    noise_loss = adversarial_loss(F.log_softmax(fake_noise), F.softmax(z1), reduction='batchmean')\n",
    "    total_loss = 0.5 * ((0.8 * g_loss + 0.2 * noise_loss) + d_loss)\n",
    "    total_loss.backward()\n",
    "    optimizer_G.step()\n",
    "    optimizer_noise.step()\n",
    "\n",
    "optim_g_sched.step()\n",
    "optim_d_sched.step()\n",
    "optim_noise_sched.step()\n",
    "\n",
    "print(f\"Epoch [{epoch+1}/{num_epochs}]  Discriminator Loss: {d_loss.item():.4f}  Generator Loss: {g_loss.item():.4f}\")\n",
    "experiment.log_metric('Main Generator\\Test', g_loss, step=epoch)\n",
    "experiment.log_metric('Noise Generator\\Test', noise_loss, step=epoch)\n",
    "experiment.log_metric('Discriminator\\Test', d_loss, step=epoch)\n",
    "experiment.log_metric('Total\\Test', total_loss, step=epoch)\n",
    "# log_model(experiment, model=main_gen, model_name=\"Main Generator\")\n",
    "# log_model(experiment, model=noise_gen, model_name=\"Noise Generator\")\n",
    "# log_model(experiment, model=discriminator, model_name=\"Discriminator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    batch_start = 0\n",
    "    total_loss_list = []\n",
    "    g_loss_list = []\n",
    "    noise_loss_list = []\n",
    "    d_loss_list = []\n",
    "    for idx in range(int(len(data)/batch_size)-1):\n",
    "        price = torch.tensor(data[batch_start:batch_start+batch_size].close, dtype=torch.float).reshape(batch_size,1)\n",
    "        price = price.to(device)\n",
    "        batch_start = batch_start + batch_size\n",
    "        # Generate random noise inputs\n",
    "        z1 = cauchy_dist.sample([batch_size, 1])\n",
    "        z1 = z1.to(device)\n",
    "        #z2 = cauchy_dist.sample([1])\n",
    "        \n",
    "        # Generate fake data\n",
    "        fake_main = main_gen(price)\n",
    "        fake_noise = noise_gen(z1)\n",
    "        fake_data = fake_main + fake_noise\n",
    "        \n",
    "        # Train discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss = adversarial_loss(discriminator(fake_data.detach()), price, reduction='batchmean')\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train generators\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_main = main_gen(price)\n",
    "        fake_noise = noise_gen(z1)\n",
    "        fake_data = fake_main + fake_noise\n",
    "        d_loss = adversarial_loss(F.log_softmax(discriminator(fake_data)), F.softmax(price), reduction='batchmean')\n",
    "        g_loss = adversarial_loss(F.log_softmax(fake_data), F.softmax(price), reduction='batchmean')\n",
    "        noise_loss = adversarial_loss(F.log_softmax(fake_noise), F.softmax(z1), reduction='batchmean')\n",
    "        total_loss = 0.5 * ((0.8 * g_loss + 0.2 * noise_loss) + d_loss)\n",
    "        total_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_noise.step()\n",
    "\n",
    "        total_loss_list.append(total_loss.item())\n",
    "        g_loss_list.append(g_loss.item())\n",
    "        noise_loss_list.append(noise_loss.item())\n",
    "        d_loss_list.append(d_loss.item())\n",
    "\n",
    "    optim_g_sched.step()\n",
    "    optim_d_sched.step()\n",
    "    optim_noise_sched.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]  Discriminator Loss: {d_loss.item():.4f}  Generator Loss: {g_loss.item():.4f}\")\n",
    "    experiment.log_metric('Main Generator\\Whole Train', np.average(g_loss_list), step=epoch)\n",
    "    experiment.log_metric('Noise Generator\\Whole Train', np.average(noise_loss_list), step=epoch)\n",
    "    experiment.log_metric('Discriminator\\Whole Train', np.average(d_loss_list), step=epoch)\n",
    "    experiment.log_metric('Total\\Whole Train', np.average(total_loss_list), step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(main_gen, 'checkpoints/main_gen_xau.pth')\n",
    "torch.save(noise_gen, 'checkpoints/noise_gen_xau.pth')\n",
    "torch.save(discriminator, 'checkpoints/discriminator_xau.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
