{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import numpy as np\n",
    "from capi import api_key\n",
    "\n",
    "from models.gan_models import *\n",
    "import yfinance as yf\n",
    "import MetaTrader5 as mt5\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api_key = os.environ['COMET_API_KEY']\n",
    "#api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/artaasd95/tail-price/c0796ccd7b334eb2a0bd399521c9a8df\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Discriminator\\Whole Train [11]  : (0.2500037994495658, 2903.2679585454075)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Main Generator\\Whole Train [11] : (0.24998955896427466, 2909.2278901744785)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [202]                      : (0.006200707517564297, 3570.6318359375)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (53.16 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/artaasd95/tail-price/2ea2808c9bcb43dca53386e8e9869302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "  api_key=api_key,\n",
    "  project_name=\"tail-price\",\n",
    "  workspace=\"artaasd95\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading current tf data\n"
     ]
    }
   ],
   "source": [
    "mt5.initialize()\n",
    "\n",
    "print('loading current tf data')\n",
    "utc_from = datetime(2021, 1, 1, tzinfo=pytz.timezone(\"Asia/Nicosia\"))\n",
    "utc_to = datetime.now(pytz.timezone(\"Asia/Nicosia\"))\n",
    "\n",
    "data = mt5.copy_rates_range('XAUUSD', mt5.TIMEFRAME_H4, utc_from, utc_to)\n",
    "data = pd.DataFrame(data)\n",
    "time_data = data.time\n",
    "data.drop(columns=['tick_volume', 'spread', 'real_volume'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('xau_2021_H4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('xau_2021_H4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1609718400</td>\n",
       "      <td>1904.48</td>\n",
       "      <td>1918.60</td>\n",
       "      <td>1900.62</td>\n",
       "      <td>1916.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1609732800</td>\n",
       "      <td>1916.65</td>\n",
       "      <td>1925.15</td>\n",
       "      <td>1915.44</td>\n",
       "      <td>1921.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1609747200</td>\n",
       "      <td>1921.52</td>\n",
       "      <td>1935.09</td>\n",
       "      <td>1921.34</td>\n",
       "      <td>1932.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1609761600</td>\n",
       "      <td>1931.95</td>\n",
       "      <td>1942.06</td>\n",
       "      <td>1927.79</td>\n",
       "      <td>1939.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1609776000</td>\n",
       "      <td>1939.78</td>\n",
       "      <td>1944.33</td>\n",
       "      <td>1929.23</td>\n",
       "      <td>1936.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5582</th>\n",
       "      <td>5582</td>\n",
       "      <td>1723780800</td>\n",
       "      <td>2458.38</td>\n",
       "      <td>2459.97</td>\n",
       "      <td>2450.72</td>\n",
       "      <td>2452.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5583</th>\n",
       "      <td>5583</td>\n",
       "      <td>1723795200</td>\n",
       "      <td>2452.68</td>\n",
       "      <td>2464.50</td>\n",
       "      <td>2451.11</td>\n",
       "      <td>2462.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5584</th>\n",
       "      <td>5584</td>\n",
       "      <td>1723809600</td>\n",
       "      <td>2462.32</td>\n",
       "      <td>2492.34</td>\n",
       "      <td>2461.17</td>\n",
       "      <td>2491.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5585</th>\n",
       "      <td>5585</td>\n",
       "      <td>1723824000</td>\n",
       "      <td>2491.46</td>\n",
       "      <td>2500.08</td>\n",
       "      <td>2477.46</td>\n",
       "      <td>2495.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586</th>\n",
       "      <td>5586</td>\n",
       "      <td>1723838400</td>\n",
       "      <td>2495.84</td>\n",
       "      <td>2509.68</td>\n",
       "      <td>2493.12</td>\n",
       "      <td>2506.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5587 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        time     open     high      low    close\n",
       "0              0  1609718400  1904.48  1918.60  1900.62  1916.57\n",
       "1              1  1609732800  1916.65  1925.15  1915.44  1921.56\n",
       "2              2  1609747200  1921.52  1935.09  1921.34  1932.03\n",
       "3              3  1609761600  1931.95  1942.06  1927.79  1939.75\n",
       "4              4  1609776000  1939.78  1944.33  1929.23  1936.92\n",
       "...          ...         ...      ...      ...      ...      ...\n",
       "5582        5582  1723780800  2458.38  2459.97  2450.72  2452.68\n",
       "5583        5583  1723795200  2452.68  2464.50  2451.11  2462.32\n",
       "5584        5584  1723809600  2462.32  2492.34  2461.17  2491.49\n",
       "5585        5585  1723824000  2491.46  2500.08  2477.46  2495.82\n",
       "5586        5586  1723838400  2495.84  2509.68  2493.12  2506.82\n",
       "\n",
       "[5587 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 512\n",
    "seq_length = 50\n",
    "num_layers = 8\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "lambda_gp = 10\n",
    "n_critic = 5\n",
    "# Initialize models\n",
    "main_gen = MainGenerator(input_size, hidden_size, num_layers, 0.5, 1)\n",
    "noise_gen = NoiseGenerator(input_size, 512, 1, 0.5)\n",
    "discriminator = Discriminator(input_size, hidden_size, num_layers, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.tensor(np.random.random((real_samples.size(0), 1)), dtype=torch.float)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones(batch_size, 1)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (lstm): LSTM(1, 512, num_layers=8, batch_first=True, dropout=0.5)\n",
       "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_gen.to(device)\n",
    "noise_gen.to(device)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.AdamW(list(main_gen.parameters()) + list(noise_gen.parameters()), lr=learning_rate)\n",
    "optimizer_D = torch.optim.AdamW(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "optim_g_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer_G, 0.1)\n",
    "optim_d_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer_D, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cauchy_dist = torch.distributions.cauchy.Cauchy(loc=0, scale=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [01:35<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]  Discriminator Loss: 4.9469789682432666  Generator Loss: 555.5506733734575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m optim_d_sched\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  Discriminator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39maverage(d_loss_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  Generator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39maverage(g_loss_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m experiment\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Generator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWhole Train\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39maverage(g_loss_list), step\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[0;32m     71\u001b[0m experiment\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiscriminator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWhole Train\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39maverage(d_loss_list), step\u001b[38;5;241m=\u001b[39mepoch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    batch_start = 0\n",
    "    g_loss_list = []\n",
    "    noise_loss_list = []\n",
    "    d_loss_list = []\n",
    "    for idx in range(int(len(data)/batch_size)-1):\n",
    "        price = torch.tensor(data[batch_start:batch_start+batch_size].close.values, dtype=torch.float).reshape(batch_size,1)\n",
    "        #price = torch.tensor(random.sample(list(data[batch_start:batch_start+batch_size].close.values), batch_size), dtype=torch.float).reshape(batch_size,1)\n",
    "        price = price.to(device)\n",
    "        batch_start = batch_start + batch_size\n",
    "        # Generate random noise inputs\n",
    "        z1 = cauchy_dist.sample([batch_size, 1])\n",
    "        z1 = z1.to(device)\n",
    "        #z2 = cauchy_dist.sample([1])\n",
    "        valid = torch.ones(batch_size, 1, device=device)\n",
    "        fake = torch.zeros(batch_size, 1, device=device)\n",
    "        # Generate fake data\n",
    "        #fake_main = main_gen(price)\n",
    "        fake_main = main_gen(z1)\n",
    "        fake_noise = noise_gen(z1)\n",
    "        fake_data = fake_main + fake_noise\n",
    "        \n",
    "        # Train disc\n",
    "        optimizer_D.zero_grad()\n",
    "        fake_main = main_gen(z1)\n",
    "        fake_noise = noise_gen(z1)\n",
    "        fake_data = fake_main + fake_noise\n",
    "\n",
    "        # Real images\n",
    "        real_validity = discriminator(price)\n",
    "        # Fake images\n",
    "        fake_validity = discriminator(fake_data)\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, price, fake_data)\n",
    "        # Adversarial loss\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "        d_loss.backward()\n",
    "        d_loss_list.append(d_loss.item())\n",
    "        optimizer_D.step()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        # Train the generator every n_critic steps\n",
    "        if idx % n_critic == 0:\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            fake_main = main_gen(z1)\n",
    "            fake_noise = noise_gen(z1)\n",
    "            fake_data = fake_main + fake_noise\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            # Train on fake data\n",
    "            fake_validity = discriminator(fake_data)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "            g_loss.backward()\n",
    "            g_loss_list.append(g_loss.item())\n",
    "            optimizer_G.step()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    optim_g_sched.step()\n",
    "    optim_d_sched.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]  Discriminator Loss: {np.average(d_loss_list)}  Generator Loss: {np.average(g_loss_list)}\")\n",
    "    experiment.log_metric('Main Generator\\Whole Train', np.average(g_loss_list), step=epoch)\n",
    "    experiment.log_metric('Discriminator\\Whole Train', np.average(d_loss_list), step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/artaasd95/tail-price/9f49e2d15e38475cabcfe319809b5eec\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Discriminator\\Whole Train [50]  : (0.25955625224945156, 3416.939735186308)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Main Generator\\Whole Train [50] : (0.26196861267089844, 3422.574053081056)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [860]                      : (0.2567949593067169, 10879.6005859375)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (37.64 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(main_gen, 'checkpoints/main_gen_xau.pth')\n",
    "torch.save(noise_gen, 'checkpoints/noise_gen_xau.pth')\n",
    "torch.save(discriminator, 'checkpoints/discriminator_xau.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
